{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3b2fef-4a08-4660-9832-6e66b68a12a1",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT2 to generator Chinese poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84762292-dec9-40dd-9a1f-6a3ee701179f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64503ad3-f231-49fb-91fb-935499d042d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline, GPT2Config\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch.optim import AdamW\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec0fddb-65ce-4f1a-b039-ba19f9d0f643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device {device}\")\n",
    "\n",
    "model_name = \"uer/gpt2-distil-chinese-cluecorpussmall\"  \n",
    "model_save_path = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c95b2f-4063-46ed-ac87-23b14e674035",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1226e79f-da50-4b94-9ce3-94344bcc0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: 深 度 学 习.? 大 一 新 生 大 一 新 生 大 一 新 生 ， 大 一 刚 入 学 ， 目 前 学 的 是 计 算 机 。 最 近 在 用 python 做 的 一 些 应 用 ， 对 于 我 来 说 很 快 的 就 能 用 。 想 用 python 做 一 些 小 游 戏 ， 不 过 对 于 做 了 一 个 月 左 右 就 可 以 做 的 很 熟 练 了 ， 然 后 再 自 学 一 些 语 法 。 因 为 在 自 学 过 程 中 我 发 现 了 很 多 可 能...\n",
      "  ---\n",
      "1: 深 度 学 习 有 点 深 了 但 是 有 点 大 有 时 候 会 把 所 有 的 知 识 点 都 记 进 去 ~ ~ 有 时 候 不 知 道 记 起 来 会 不 会 忘 记 ~ ~ ~ ~ ~ ~ ~ 有 时 候 记 起 来 会 记 不 得 东 西 啊 ~ ~ ~ ~ ~ ~ ~ 。 有 时 候 记 起 来 会 忘 掉 的 ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ 。 有 时 候 记 得 会 忘 掉...\n",
      "  ---\n",
      "2: 深 度 学 习 还 可 以 这 本 书 的 内 容 不 是 很 深 但 也 不 是 说 很 难 但 是 看 完 了 一 遍 以 后 你 会 发 现 有 不 少 地 方 还 是 蛮 不 错 的 。 这 个 作 者 的 作 品 里 面 我 还 可 以 看 到 了 很 多 相 关 的 知 识 在 这 个 学 习 的 过 程 中 你 会 发 现 很 多 不 同 的 知 识 。 希 望 大 家 能 给 自 己 一 些 帮 助 。 。 。...\n",
      "  ---\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input_sequence = \"深度学习\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "model = model.to(device)\n",
    "#combine both sampling techniques\n",
    "sample_outputs = model.generate(\n",
    "                              input_ids.to(device),\n",
    "                              do_sample = True,\n",
    "                              max_length = 120,\n",
    "                              top_k = 50,\n",
    "                              top_p = 0.85,\n",
    "                              num_return_sequences = 3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('  ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919f4e9-08a2-4e65-9ddd-9ac1109a53a4",
   "metadata": {},
   "source": [
    "## Prepare Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a61d4185-7baf-4278-9cdd-8ff64748b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开。\n",
      "初风飘带柳，晚雪间花梅。\n",
      "碧林青旧竹，绿沼翠新苔。\n",
      "芝田初雁去，绮树巧莺来。\n",
      "晚霞聊自怡，初晴弥可喜。\n",
      "日晃百花色，风动千林翠。\n",
      "池鱼跃不同，园鸟声还异。\n",
      "寄言博通者，知予物\n"
     ]
    }
   ],
   "source": [
    "with open('./poetry.txt', 'r') as f:\n",
    "    poetry_corpus = f.read()\n",
    "print(poetry_corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570bf4c7-91d9-4ead-98c1-27df26f47a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寒随穷律变，春逐鸟声开初风飘带柳，晚雪间花梅碧林青旧竹，绿沼翠新苔芝田初雁去，绮树巧莺来晚霞聊自怡，初晴弥可喜日晃百花色，风动千林翠池鱼跃不同，园鸟声还异寄言博通者，知予物外志一朝春夏改，隔夜鸟花迁阴\n"
     ]
    }
   ],
   "source": [
    "poetry_corpus = poetry_corpus.replace('\\n', '').replace('\\r', '').replace('。', '')\n",
    "print(poetry_corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f339c38a-b251-4b7a-aeb0-142f3d5301d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s, n=11):\n",
    "  \"\"\"将字符串s分割成长度为n的子字符串列表\n",
    "  Args:\n",
    "    s: 要分割的字符串\n",
    "    n: 每个子字符串的长度\n",
    "  Returns:\n",
    "    分割后的子字符串列表\n",
    "  \"\"\"\n",
    "  return [s[i:i+n] for i in range(0, len(s), n)]\n",
    "text = poetry_corpus \n",
    "text = str_to_list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff7dc1d2-7d5a-4232-a12e-92cfa7b329d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72514\n",
      "朱颜含远日，翠色影长津\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0816f4f-ca80-479a-9569-81bb424d6ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|>朱颜含远日，翠色影长津<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "def add_special_chars(list_of_strings, left_char, right_char):\n",
    "  \"\"\"\n",
    "  在列表中每个字符串的开头和结尾添加特殊字符\n",
    "  Args:\n",
    "    list_of_strings: 字符串列表\n",
    "    special_char: 要添加的特殊字符\n",
    "  Returns:\n",
    "    添加了特殊字符的新列表\n",
    "  \"\"\"\n",
    "  return [left_char + s + right_char for s in list_of_strings]\n",
    "\n",
    "text = add_special_chars(text, left_char='<|startoftext|>', right_char='<|endoftext|>')\n",
    "print(text[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36b5ad-b00b-4e33-9e0d-36d5c5e19c80",
   "metadata": {},
   "source": [
    "## GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952805c9-6b3a-40ec-81cc-2042b9364db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name,bos_token='<|startoftext|>',eos_token='<|endoftext|>',unk_token='<|unknown|>',pad_token='<|pad|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5317ff1-b860-474a-ba41-498dafe83bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(tokenizer.vocab.items(), key=lambda x:x[1])\n",
    "print(len(vocab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0eef55b-b248-4232-8199-b18b6f008133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model\n",
      "The end of sequence token <|endoftext|> has the id 21129\n",
      "The beginning of sequence token <|startoftext|> has the id 21128\n",
      "The unknown token <|unknown|> has the id 21130\n",
      "The padding token <|pad|> has the id 21131\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The beginning of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The unknown token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.unk_token_id), tokenizer.unk_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03304b-2fbf-4596-9424-8b86141743b3",
   "metadata": {},
   "source": [
    "## PyTorch Datasets & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7791fa-e203-4514-a177-87adc44f5f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65,262 training samples\n",
      "7,252 validation samples\n"
     ]
    }
   ],
   "source": [
    "# GPT2 is a large model. Increasing the batch size above 2 has lead to out of memory problems.\n",
    "batch_size = 16\n",
    "max_length = 180  # maximum sentence length\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        # 使用tokenizer   \n",
    "        encoding = self.tokenizer(text, truncation=True, return_tensors='pt')\n",
    "        return encoding\n",
    "        \n",
    "dataset = MyDataset(text, tokenizer)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e46686d-06c5-482f-9cbc-1f4384ae0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 72514\n",
      "{'input_ids': tensor([[  101, 21128,  2170,  7390,  4956,  2526,  1359,  8024,  3217,  6852,\n",
      "          7881,  1898,  2458, 21129,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "dataset[0]: \n",
      "  input_ids: tensor([[  101, 21128,  2170,  7390,  4956,  2526,  1359,  8024,  3217,  6852,\n",
      "          7881,  1898,  2458, 21129,   102]])\n",
      "  attn_masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset size {dataset.__len__()}\")\n",
    "print(dataset[0])\n",
    "print(f\"dataset[0]: \\n  input_ids: {dataset[0].input_ids}\\n  attn_masks: {dataset[0].attention_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34de3306-a571-4ea8-b195-b44f61a51126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722341f0-9fdf-40e8-a094-c95bdb23a68c",
   "metadata": {},
   "source": [
    "## Finetune GPT2 Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5acb0b57-dbec-4aa3-919d-c17bdbd4b64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape torch.Size([21128, 768])\n",
      "Number of tokens: 21132\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_name, output_hidden_states=False)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "model = model.to(device)\n",
    "print(f\"Weight shape {model.transformer.wte.weight.shape}\")\n",
    "# this step is necessary because I've added some tokens (bos_token, etc.) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Number of tokens: {len(tokenizer)}\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98152195-bbc6-4c5e-b738-f94e6d3d48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21132, 768])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = model.transformer.wte.weight # Word Token Embeddings\n",
    "\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f22b546-db4a-4d79-ad3c-1763d3606775",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 1e2\n",
    "# The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n",
    "epsilon = 1e-8\n",
    "# optim = Adam(model.parameters(), lr=5e-5)\n",
    "optim = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a82b4dac-eb7f-4e55-8821-f75f741305a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b0105a7-8a0e-4581-a04c-eb46222f937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_max_length = 24\n",
    "\n",
    "def infer(prompt):\n",
    "    # input = f\"<|startoftext|>{prompt.strip()}\"\n",
    "    # input = f\"<|startoftext|> {prompt.strip()}\"\n",
    "    input = f\"{prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=infer_max_length,\n",
    "                            # temperature = 0.5,\n",
    "                            do_sample = True, top_k = 50, top_p = 0.85)\n",
    "                            # num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad132419-58f3-4322-8b83-2b9e3ffb6ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   500  of  4,079. Loss: 4.796802520751953.   Elapsed: 0:00:10.\n",
      "门 外 见 月 明 ， 何 处 何 处 有 扉 花\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  4,079. Loss: 4.5659708976745605.   Elapsed: 0:00:20.\n",
      "闻 云 上 香 ， 何 人 去 多 年 相 惜 天 涯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,500  of  4,079. Loss: 4.4266276359558105.   Elapsed: 0:00:30.\n",
      "觉 天 堂 有 心 ， 天 地 万 里 开 啼 来 思 旧\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  4,079. Loss: 4.273077964782715.   Elapsed: 0:00:40.\n",
      "去 到 白 发 白 ， 长 白 似 苍 苍\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,500  of  4,079. Loss: 4.059493541717529.   Elapsed: 0:00:49.\n",
      "日 无 处 看 雨 ， 独 自 望 春 城\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  4,079. Loss: 4.371115684509277.   Elapsed: 0:00:59.\n",
      "地 不 成 功 ， 风 雨 无 生 涯 前\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,500  of  4,079. Loss: 4.077295780181885.   Elapsed: 0:01:09.\n",
      "闲 听 闻 此 地 ， 未 曾 发 一 人\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4,000  of  4,079. Loss: 4.264497756958008.   Elapsed: 0:01:18.\n",
      "闻 香 客 后 路 ， 犹 忆 去 时 情\n",
      "\n",
      "  Average training loss: 4.47\n",
      "  Training epoch took: 0:01:20\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.45\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   500  of  4,079. Loss: 4.2762885093688965.   Elapsed: 0:00:10.\n",
      "知 君 有 子 ， 知 君 为 此 悲 ， 何 处 惜 家 贫\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  4,079. Loss: 3.974090814590454.   Elapsed: 0:00:19.\n",
      "秋 水 上 船 ， 飞 渡 出 山 村\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,500  of  4,079. Loss: 3.7294118404388428.   Elapsed: 0:00:29.\n",
      "白 松 萝 径 ， 空 悬 落 日 舟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  4,079. Loss: 4.095635890960693.   Elapsed: 0:00:38.\n",
      "知 何 时 尽 ， 相 看 一 自 惊 归\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,500  of  4,079. Loss: 4.010121822357178.   Elapsed: 0:00:48.\n",
      "花 满 树 枝 落 ， 不 如 白 云 飞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  4,079. Loss: 3.901571750640869.   Elapsed: 0:00:58.\n",
      "花 飘 雨 雪 ， 远 落 花 风\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,500  of  4,079. Loss: 3.830249309539795.   Elapsed: 0:01:07.\n",
      "春 多 白 羽 ， 春 满 若 青 枫\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4,000  of  4,079. Loss: 3.852318048477173.   Elapsed: 0:01:17.\n",
      "知 何 去 岁 寒 ， 却 见 一 时 声\n",
      "\n",
      "  Average training loss: 4.00\n",
      "  Training epoch took: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.15\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   500  of  4,079. Loss: 4.047907829284668.   Elapsed: 0:00:10.\n",
      "地 通 灵 泽 ， 时 时 解 帝 乡 思\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  4,079. Loss: 4.130640506744385.   Elapsed: 0:00:19.\n",
      "日 下 人 心 静 ， 空 山 独 一 闻\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,500  of  4,079. Loss: 4.231122970581055.   Elapsed: 0:00:29.\n",
      "知 尔 未 遂 至 ， 长 啸 不 相 关\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  4,079. Loss: 4.123702526092529.   Elapsed: 0:00:39.\n",
      "不 足 日 ， 无 语 与 谁 同\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,500  of  4,079. Loss: 3.8870315551757812.   Elapsed: 0:00:48.\n",
      "风 来 后 路 ， 千 里 故 乡 心\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  4,079. Loss: 4.07708740234375.   Elapsed: 0:00:58.\n",
      "知 非 君 子 心 ， 犹 自 感 恩 悲\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,500  of  4,079. Loss: 3.8599071502685547.   Elapsed: 0:01:08.\n",
      "去 年 秋 日 里 ， 江 树 绿 新 枝\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4,000  of  4,079. Loss: 3.9179937839508057.   Elapsed: 0:01:17.\n",
      "地 是 乡 思 远 ， 愁 云 在 北 楼\n",
      "\n",
      "  Average training loss: 3.88\n",
      "  Training epoch took: 0:01:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 4.06\n",
      "  Validation took: 0:00:03\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:04:07 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()  # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):     # step from enumerate() = number of batches\n",
    "\n",
    "        b_input_ids = batch.input_ids.to(device)   # tokens (of multiple documents in a batch)\n",
    "        b_labels    = batch.input_ids.to(device)\n",
    "        b_masks     = batch.attention_mask.to(device)   # mask of [1] for a real word, [0] for a pad\n",
    "\n",
    "        model.zero_grad()\n",
    "        # loss = model(X.to(device), attention_mask=a.to(device), labels=X.to(device)).loss\n",
    "        outputs = model(  input_ids = b_input_ids,\n",
    "                          labels = b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids = None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_output = infer(\"\")\n",
    "            print(sample_output)\n",
    "\n",
    "            # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch.input_ids.to(device)\n",
    "        b_labels = batch.input_ids.to(device)\n",
    "        b_masks = batch.attention_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(input_ids = b_input_ids,\n",
    "                             attention_mask = b_masks,\n",
    "                             labels = b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f107df95-3485-4387-925e-4616ea4da004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.465735</td>\n",
       "      <td>4.445215</td>\n",
       "      <td>0:01:20</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.004158</td>\n",
       "      <td>4.153035</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.882568</td>\n",
       "      <td>4.055147</td>\n",
       "      <td>0:01:19</td>\n",
       "      <td>0:00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss Training Time Validation Time\n",
       "epoch                                                          \n",
       "1           4.465735     4.445215       0:01:20         0:00:03\n",
       "2           4.004158     4.153035       0:01:19         0:00:03\n",
       "3           3.882568     4.055147       0:01:19         0:00:03"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d8edb-4659-4d34-8d5a-2861a7fa9764",
   "metadata": {},
   "source": [
    "## Saving & Loading Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70dbbfa6-8d52-45b3-993e-7a55c2951929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.txt',\n",
       " './model/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Saving model to %s\" % model_save_path)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "467b2365-614f-4ef1-a93f-20e39798196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21132, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21132, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344bf09-0c07-43b7-8daa-74fb4347f005",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4b441c-2e8e-4582-8c37-8fec002e45bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有 佳 句 来 ， 空 馀 古 道 心\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu_dhj_1",
   "language": "python",
   "name": "pytorch-gpu_dhj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
